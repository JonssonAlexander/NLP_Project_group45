{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b68f06b",
   "metadata": {},
   "source": [
    "\n",
    "# 03 - Modular Enhancements\n",
    "\n",
    "This notebook mirrors the Part 3 experiments but keeps the code lightweight by\n",
    "relying on helper modules in `src/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116952d",
   "metadata": {},
   "source": [
    "## Imports & shared setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d6aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "from IPython.display import Markdown, display\n",
    "from dataclasses import replace, dataclass\n",
    "\n",
    "from src.config import load_data_config\n",
    "from src.dataset_pipeline import prepare_tokenised_splits\n",
    "from src.embeddings import load_torchtext_glove\n",
    "from src.reports import build_vocabulary_report\n",
    "from src.models.birnn import BiLSTMClassifier, BiGRUClassifier\n",
    "from src.models.cnn import CNNTextClassifier\n",
    "from src.training import (\n",
    "    RNNExperimentConfig,\n",
    "    train_rnn_model,\n",
    "    build_dataloaders_and_vocab,\n",
    "    run_model_experiment,\n",
    "    summarise_run,\n",
    ")\n",
    "from src.evaluation import evaluate_model, topic_accuracy_table, classification_report_table\n",
    "from src.plotting import plot_training_curves\n",
    "\n",
    "SEED = 7\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de35ea",
   "metadata": {},
   "source": [
    "## Data & embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f9ac5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4361, 1091, 500)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config = load_data_config(Path('configs/data.yaml'))\n",
    "splits = prepare_tokenised_splits(data_config)\n",
    "len(splits.train), len(splits.validation), len(splits.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20dce0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_report = build_vocabulary_report(\n",
    "    tokenised_dataset=splits.train,\n",
    "    min_freq=data_config.vocabulary_min_freq,\n",
    "    specials=data_config.vocabulary_specials,\n",
    ")\n",
    "embedding_result = load_torchtext_glove(\n",
    "    vocabulary=vocab_report.vocabulary,\n",
    "    name='6B',\n",
    "    dim=100,\n",
    "    trainable=True,\n",
    "    random_seed=SEED,\n",
    ")\n",
    "embedding_tensor = torch.tensor(embedding_result.matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a45bd",
   "metadata": {},
   "source": [
    "## Shared experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed0144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = RNNExperimentConfig(\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    dropout=0.3,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    grad_clip=1.0,\n",
    "    pooling='last_hidden',\n",
    "    optimizer='adam',\n",
    "    early_stopping_patience=3,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loaders, label_to_index = build_dataloaders_and_vocab(base_config, splits, vocab_report.vocabulary)\n",
    "records = {'rows': [], 'histories': {}, 'evaluations': {}, 'label_maps': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7dc8c",
   "metadata": {},
   "source": [
    "## Baseline RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_hist, b_model, b_label_map, b_loaders = train_rnn_model(\n",
    "    base_config,\n",
    "    splits=splits,\n",
    "    vocabulary=vocab_report.vocabulary,\n",
    "    embedding_result=embedding_result,\n",
    "    device=device,\n",
    ")\n",
    "b_eval = evaluate_model(b_model, b_loaders.test, device=device)\n",
    "records['histories']['rnn_baseline'] = b_hist\n",
    "records['evaluations']['rnn_baseline'] = b_eval\n",
    "records['label_maps']['rnn_baseline'] = b_label_map\n",
    "records['rows'].append(summarise_run('rnn_baseline', b_hist, b_eval.accuracy))\n",
    "plot_training_curves(\n",
    "    b_hist,\n",
    "    title='RNN baseline training dynamics',\n",
    "    output_path=Path('plots/part3_rnn_baseline_curves.png'),\n",
    ")\n",
    "display(Markdown(f\"Baseline test accuracy: {b_eval.accuracy:.3f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39a5cd",
   "metadata": {},
   "source": [
    "## biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_bilstm, model_bilstm, eval_bilstm = run_model_experiment(\n",
    "    'bilstm',\n",
    "    model_builder=lambda: BiLSTMClassifier(\n",
    "        embedding_matrix=embedding_tensor.clone(),\n",
    "        num_classes=len(label_to_index),\n",
    "        hidden_dim=base_config.hidden_dim,\n",
    "        num_layers=base_config.num_layers,\n",
    "        dropout=base_config.dropout,\n",
    "    ),\n",
    "    config=base_config,\n",
    "    loaders=loaders,\n",
    "    device=device,\n",
    ")\n",
    "records['histories']['bilstm'] = hist_bilstm\n",
    "records['evaluations']['bilstm'] = eval_bilstm\n",
    "records['label_maps']['bilstm'] = {idx: label for label, idx in label_to_index.items()}\n",
    "records['rows'].append(summarise_run('bilstm', hist_bilstm, eval_bilstm.accuracy))\n",
    "display(Markdown(f\"biLSTM test accuracy: {eval_bilstm.accuracy:.3f}\"))\n",
    "plot_training_curves(\n",
    "    hist_bilstm,\n",
    "    title='biLSTM training dynamics',\n",
    "    output_path=Path('plots/part3_bilstm_curves.png'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6f3cd",
   "metadata": {},
   "source": [
    "## biGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_bigru, model_bigru, eval_bigru = run_model_experiment(\n",
    "    'bigru',\n",
    "    model_builder=lambda: BiGRUClassifier(\n",
    "        embedding_matrix=embedding_tensor.clone(),\n",
    "        num_classes=len(label_to_index),\n",
    "        hidden_dim=base_config.hidden_dim,\n",
    "        num_layers=base_config.num_layers,\n",
    "        dropout=base_config.dropout,\n",
    "    ),\n",
    "    config=base_config,\n",
    "    loaders=loaders,\n",
    "    device=device,\n",
    ")\n",
    "records['histories']['bigru'] = hist_bigru\n",
    "records['evaluations']['bigru'] = eval_bigru\n",
    "records['label_maps']['bigru'] = {idx: label for label, idx in label_to_index.items()}\n",
    "records['rows'].append(summarise_run('bigru', hist_bigru, eval_bigru.accuracy))\n",
    "display(Markdown(f\"biGRU test accuracy: {eval_bigru.accuracy:.3f}\"))\n",
    "plot_training_curves(\n",
    "    hist_bigru,\n",
    "    title='biGRU training dynamics',\n",
    "    output_path=Path('plots/part3_bigru_curves.png'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4f66d",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee129465",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_config = replace(base_config, dropout=0.5, weight_decay=0.0, grad_clip=0.0)\n",
    "hist_cnn, model_cnn, eval_cnn = run_model_experiment(\n",
    "    'cnn',\n",
    "    model_builder=lambda: CNNTextClassifier(\n",
    "        embedding_matrix=embedding_tensor.clone(),\n",
    "        num_classes=len(label_to_index),\n",
    "        dropout=cnn_config.dropout,\n",
    "    ),\n",
    "    config=cnn_config,\n",
    "    loaders=loaders,\n",
    "    device=device,\n",
    ")\n",
    "records['histories']['cnn'] = hist_cnn\n",
    "records['evaluations']['cnn'] = eval_cnn\n",
    "records['label_maps']['cnn'] = {idx: label for label, idx in label_to_index.items()}\n",
    "records['rows'].append(summarise_run('cnn', hist_cnn, eval_cnn.accuracy))\n",
    "display(Markdown(f\"CNN test accuracy: {eval_cnn.accuracy:.3f}\"))\n",
    "plot_training_curves(\n",
    "    hist_cnn,\n",
    "    title='CNN training dynamics',\n",
    "    output_path=Path('plots/part3_cnn_curves.png'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65593b1f",
   "metadata": {},
   "source": [
    "## Summary & comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a444de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(records['rows']).set_index('name').sort_values('test_accuracy', ascending=False)\n",
    "display(comparison_df)\n",
    "\n",
    "best_name = comparison_df.index[0]\n",
    "best_eval = records['evaluations'][best_name]\n",
    "best_history = records['histories'][best_name]\n",
    "best_label_map = records['label_maps'][best_name]\n",
    "display(Markdown(f\"Best model: **{best_name}** (test accuracy {comparison_df.loc[best_name, 'test_accuracy']:.3f})\"))\n",
    "\n",
    "best_topic_df = topic_accuracy_table(best_eval, best_label_map)\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "accuracy_colors = [cm.get_cmap('viridis')(acc) for acc in best_topic_df['accuracy'].clip(0, 1)]\n",
    "bars = ax.bar(best_topic_df['label'], best_topic_df['support'], color=accuracy_colors)\n",
    "ax.set_xlabel('Topic')\n",
    "ax.set_ylabel('Support')\n",
    "ax.set_title(f\"{best_name.upper()} – topic-wise support (color = accuracy)\")\n",
    "ax.set_ylim(0, best_topic_df['support'].max() * 1.1)\n",
    "for bar, accuracy in zip(bars, best_topic_df['accuracy']):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5, f\"{accuracy:.2f}\", ha='center', va='bottom', fontsize=9)\n",
    "sm = plt.cm.ScalarMappable(cmap=cm.get_cmap('viridis'), norm=colors.Normalize(0, 1))\n",
    "sm.set_array([])\n",
    "fig.colorbar(sm, ax=ax, label='Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"plots/part3_{best_name}_topic_accuracy.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "best_report_df = classification_report_table(best_eval, best_label_map)\n",
    "label_rows = best_report_df.loc[~best_report_df.index.isin(['accuracy', 'macro avg', 'weighted avg', 'micro avg'])].copy()\n",
    "if 'accuracy' not in label_rows.columns:\n",
    "    label_rows['accuracy'] = label_rows.get('recall', 0.0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "support_colors = [cm.get_cmap('magma')(acc) for acc in label_rows['accuracy'].clip(0, 1)]\n",
    "bars = ax.bar(label_rows.index, label_rows['support'], color=support_colors)\n",
    "ax.set_xlabel('Topic')\n",
    "ax.set_ylabel('Support')\n",
    "ax.set_title(f\"{best_name.upper()} – support by topic (color = accuracy)\")\n",
    "ax.set_ylim(0, label_rows['support'].max() * 1.1)\n",
    "for bar, accuracy in zip(bars, label_rows['accuracy']):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5, f\"{accuracy:.2f}\", ha='center', va='bottom', fontsize=9)\n",
    "sm = plt.cm.ScalarMappable(cmap=cm.get_cmap('magma'), norm=colors.Normalize(0, 1))\n",
    "sm.set_array([])\n",
    "fig.colorbar(sm, ax=ax, label='Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"plots/part3_{best_name}_classification_support.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "f1_colors = [cm.get_cmap('plasma')(score) for score in label_rows['f1-score'].clip(0, 1)]\n",
    "bars = ax.bar(label_rows.index, label_rows['f1-score'], color=f1_colors)\n",
    "ax.set_xlabel('Topic')\n",
    "ax.set_ylabel('F1 score')\n",
    "ax.set_title(f\"{best_name.upper()} – F1 score by topic\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "for bar, score in zip(bars, label_rows['f1-score']):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02, f\"{score:.2f}\", ha='center', va='bottom', fontsize=9)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"plots/part3_{best_name}_classification_f1.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "averages_df = best_report_df.loc[\n",
    "    ['accuracy', 'macro avg', 'weighted avg'],\n",
    "    [col for col in best_report_df.columns if col in ['precision', 'recall', 'f1-score', 'support']],\n",
    "]\n",
    "display(averages_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
